{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cassandra Maldonado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:33.663563Z",
     "start_time": "2025-04-03T18:36:33.660907Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/casey/Library/Messages/Attachments/cb/11/714E94DA-852D-41B2-AA68-B482B8F9A34C'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Data Cleaning & Exploratory Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import full data set using Pandas:\n",
    "   * Verify that the shape is: (864863, 74)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.075515Z",
     "start_time": "2025-04-03T18:36:33.690286Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/casey/Library/Messages/Attachments/cb/11/714E94DA-852D-41B2-AA68-B482B8F9A34C'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bottle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.091612Z",
     "start_time": "2025-04-03T18:36:36.082420Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/casey/Library/Messages/Attachments/cb/11/714E94DA-852D-41B2-AA68-B482B8F9A34C'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.144432Z",
     "start_time": "2025-04-03T18:36:36.141936Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Limit the dataset to the following columns: [T_degC, Salnty, STheta] and then remove all NaN and NA values (Hint: Pandas has a method for this) .\n",
    "   * Verify the shape is: (812174, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.191473Z",
     "start_time": "2025-04-03T18:36:36.176636Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df[['T_degC', 'Salnty', 'STheta']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.256528Z",
     "start_time": "2025-04-03T18:36:36.254361Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Clean dataset shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build the following plots using matplotlib: \n",
    "\n",
    "a) Scatter: Salnty (salinity of water) vs T_degC \n",
    "\n",
    "b) Scatter: STheta (density of water) vs T_degC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.483495Z",
     "start_time": "2025-04-03T18:36:36.299590Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_clean['T_degC'], df_clean['Salnty'], alpha=0.5, s=1)\n",
    "plt.title('Salinity vs Temperature')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Salinity')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:36.681256Z",
     "start_time": "2025-04-03T18:36:36.503320Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_clean['T_degC'], df_clean['STheta'], alpha=0.5, s=1)\n",
    "plt.title('Density vs Temperature')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Density (STheta)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build the following plot using seaborn: \n",
    "\n",
    "a) Distribution of T_degC (hint: distplot will be helpful here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:37.959953Z",
     "start_time": "2025-04-03T18:36:36.699835Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['T_degC'], kde=True)\n",
    "plt.title('Distribution of Temperature')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('temp_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove obvious outliers from step 3b:\n",
    "   *  Shape for clean data will be: (812168, 3)\n",
    "   * Hint: Just looking to remove extreme STheta values - please post on Canvas if struggling with this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:37.994288Z",
     "start_time": "2025-04-03T18:36:37.979258Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = df_clean['STheta'].mean()\n",
    "std = df_clean['STheta'].std()\n",
    "\n",
    "lower_bound = mean - 5 * std\n",
    "upper_bound = mean + 5 * std\n",
    "\n",
    "df_no_outliers = df_clean[(df_clean['STheta'] >= lower_bound) &\n",
    "                          (df_clean['STheta'] <= upper_bound)]\n",
    "\n",
    "print(f\"Dataset shape after outlier removal: {df_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot 3b again – does it look better? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.797982Z",
     "start_time": "2025-04-03T18:36:38.012996Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_no_outliers['T_degC'], df_no_outliers['STheta'], alpha=0.5, s=1)\n",
    "plt.title('Density vs Temperature After Outlier Removal')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Density (STheta)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('density_vs_temp_after.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot with outliers the y-axis extends down to 20 kg/m3, where there appears to be some isolated points at the bottom of the plot. \n",
    "\n",
    "In the plot after removing outliers the y-axis has a narrower range from 22.5-28 kg/m3 and the main data trend is more clearly visible and the relationship between temperature and density is better visualized.\n",
    "\n",
    "The plot after removing the outliers looks better for several reasons:\n",
    "\n",
    "- By removing extreme values, the visualization focuses on the range where most data points sre, allowing patterns to be more visible.\n",
    "\n",
    "- The inverse relationship between temperature and water density is now more evident, as temperature increases, density decreases in a consistent pattern.\n",
    "\n",
    "- You can now see what appears to be different water masses or layers in the ocean, shown by the bands of density at different temperature ranges.\n",
    "\n",
    "The improvement demonstrates why outlier removal is an important step in oceanographic data analysis, as these extreme values can affect the relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Train & Test Split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, we must generate a training set for model building and a testing set for model validation. Feel free to perform these steps however you want, but please note that sklearn has a `train_test_split` class that is perfect for this type of work.\n",
    "\n",
    "\n",
    "Our X matrix (features / independent variables) will be: [Salnty, STheta]\n",
    "\n",
    "Our y matrix (target / dependent variable) will be: T_degC\n",
    "\n",
    "Build out the following:\n",
    "- X_train\n",
    "- X_test\n",
    "- y_train\n",
    "- y_test\n",
    "\n",
    "Make sure to add a seed (called random_state in train_test_split) so your split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.826923Z",
     "start_time": "2025-04-03T18:36:38.822523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature matrix X and target variable\n",
    "X = df_no_outliers[['Salnty', 'STheta']]  \n",
    "y = df_no_outliers['T_degC']               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.898312Z",
     "start_time": "2025-04-03T18:36:38.864513Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.910024Z",
     "start_time": "2025-04-03T18:36:38.907209Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original dataset shape:\", df_no_outliers.shape)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.936485Z",
     "start_time": "2025-04-03T18:36:38.929998Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining set statistics:\")\n",
    "print(\"X_train['Salnty'] mean:\", X_train['Salnty'].mean())\n",
    "print(\"X_train['STheta'] mean:\", X_train['STheta'].mean())\n",
    "print(\"y_train mean:\", y_train.mean())\n",
    "\n",
    "print(\"\\nTest set statistics:\")\n",
    "print(\"X_test['Salnty'] mean:\", X_test['Salnty'].mean())\n",
    "print(\"X_test['STheta'] mean:\", X_test['STheta'].mean())\n",
    "print(\"y_test mean:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Linear Regression Using Normal Equation - Coded In Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the closed-form solution called the Normal Equation to solve the following:\n",
    "1. Build the normal equation using numpy & fit using your training data (X_train & y_train).\n",
    "   * Note: Make sure to include an intercept value of 1 for every observation in your X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.965300Z",
     "start_time": "2025-04-03T18:36:38.957005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding a column of 1 for the intercept\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Normal Equation: (X^T X)^(-1) X^T y\n",
    "# X^T X\n",
    "XT_X = X_train_b.T.dot(X_train_b)\n",
    "\n",
    "# Iinverse of X^T X\n",
    "XT_X_inv = np.linalg.inv(XT_X)\n",
    "\n",
    "# X^T y\n",
    "XT_y = X_train_b.T.dot(y_train)\n",
    "\n",
    "# Coefficients\n",
    "theta = XT_X_inv.dot(XT_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out the coefficients - we will need to compare these to the output in Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:38.985277Z",
     "start_time": "2025-04-03T18:36:38.982918Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Linear Regression Coefficients (Normal Equation):\")\n",
    "print(f\"Intercept: {theta[0]:.4f}\")\n",
    "print(f\"Coefficient for Salinity: {theta[1]:.4f}\")\n",
    "print(f\"Coefficient for Density (STheta): {theta[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.006856Z",
     "start_time": "2025-04-03T18:36:39.003793Z"
    }
   },
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'Feature': ['Intercept', 'Salinity', 'Density (STheta)'],\n",
    "    'Coefficient': theta\n",
    "})\n",
    "print(\"\\nCoefficients table:\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Predict T_degC for the test data using the fitted values in b.\n",
    "   * Make sure to include an intercept value of 1 for every observation in your X_test.\n",
    "   * Call predictions y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.027084Z",
     "start_time": "2025-04-03T18:36:39.024525Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = X_test_b.dot(theta)\n",
    "print(\"\\nPredictions on the test set:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the following for the test data:\n",
    "   * mean squared error\n",
    "   * r-squared\n",
    "   *  explained variance\n",
    "   * Note: https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metricsLinks to an external site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.051537Z",
     "start_time": "2025-04-03T18:36:39.045447Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "exp_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R²): {r_squared:.4f}\")\n",
    "print(f\"Explained Variance Score: {exp_var:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a scatterplot that shows actual versus predicted values for the T_degC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.372041Z",
     "start_time": "2025-04-03T18:36:39.070586Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "\n",
    "# Add a perfect prediction line\n",
    "min_val = min(min(y_test), min(y_pred))\n",
    "max_val = max(max(y_test), max(y_pred))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "\n",
    "plt.title('Actual vs. Predicted Temperature Values', fontsize=15)\n",
    "plt.xlabel('Actual Temperature (°C)', fontsize=12)\n",
    "plt.ylabel('Predicted Temperature (°C)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.588948Z",
     "start_time": "2025-04-03T18:36:39.405537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Residuals Plot\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs Predicted Values', fontsize=15)\n",
    "plt.xlabel('Predicted Temperature (°C)', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.693201Z",
     "start_time": "2025-04-03T18:36:39.593660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking if residuals are normally distributed\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=50, alpha=0.7)\n",
    "plt.title('Distribution of Residuals', fontsize=15)\n",
    "plt.xlabel('Residual Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Using sklearn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.787339Z",
     "start_time": "2025-04-03T18:36:39.785260Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn is a fantastic tool within python full of various model classes. For this portion of the assignment, use the class LinearRegression to replicate work done in Part C. Specifically, you should create an instance of the following class:\n",
    "\n",
    "1. Create an instance of the LinearRegression class called \"lin_reg\". Fit lin_reg using X_train & y_train.\n",
    "   * Note: sklearn will automatically add the intercept term, so you do not need to do this here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.824219Z",
     "start_time": "2025-04-03T18:36:39.806410Z"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out the coefficients (including the intercept)\n",
    "   * Note: These should be EXACTLY the same as what you found in Part C step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.845228Z",
     "start_time": "2025-04-03T18:36:39.842731Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Linear Regression Coefficients (sklearn):\")\n",
    "print(f\"Intercept: {lin_reg.intercept_:.4f}\")\n",
    "print(f\"Coefficient for Salinity: {lin_reg.coef_[0]:.4f}\")\n",
    "print(f\"Coefficient for Density (STheta): {lin_reg.coef_[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.866220Z",
     "start_time": "2025-04-03T18:36:39.863110Z"
    }
   },
   "outputs": [],
   "source": [
    "coef_df1 = pd.DataFrame({\n",
    "    'Feature': ['Intercept', 'Salinity', 'Density (STheta)'],\n",
    "    'Coefficient': [lin_reg.intercept_, lin_reg.coef_[0], lin_reg.coef_[1]]\n",
    "})\n",
    "print(\"\\nCoefficients table:\")\n",
    "print(coef_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Predict T_degC for the test data using the fitted values in b.\n",
    "   * Call predictions y_pred_sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.888309Z",
     "start_time": "2025-04-03T18:36:39.884148Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_sklearn = lin_reg.predict(X_test)\n",
    "print(\"\\nPredictions on the test set (sklearn):\")\n",
    "print(y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the following for the test data:\n",
    "   * mean squared error\n",
    "   * r-squared\n",
    "   * explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:39.912227Z",
     "start_time": "2025-04-03T18:36:39.906803Z"
    }
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred_sklearn)\n",
    "rmse = np.sqrt(mse)\n",
    "r_squared = r2_score(y_test, y_pred_sklearn)\n",
    "exp_var = explained_variance_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R²): {r_squared:.4f}\")\n",
    "print(f\"Explained Variance Score: {exp_var:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a scatterplot that shows actual versus predicted values for the T_degC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:40.230470Z",
     "start_time": "2025-04-03T18:36:39.930968Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred_sklearn, alpha=0.5)\n",
    "\n",
    "min_val = min(min(y_test), min(y_pred_sklearn))\n",
    "max_val = max(max(y_test), max(y_pred_sklearn))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "\n",
    "plt.title('Actual vs. Predicted Temperature Values Sklearn', fontsize=15)\n",
    "plt.xlabel('Actual Temperature (°C)', fontsize=12)\n",
    "plt.ylabel('Predicted Temperature (°C)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:40.254324Z",
     "start_time": "2025-04-03T18:36:40.251145Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    manual_pred_diff = np.abs(y_pred - y_pred_sklearn).mean()\n",
    "    print(f\"\\nAverage difference between manual and sklearn predictions: {manual_pred_diff:.10f}\")\n",
    "    \n",
    "    print(\"\\nComparison of coefficients:\")\n",
    "    print(f\"Manual intercept vs sklearn intercept difference: {abs(theta[0] - lin_reg.intercept_):.10f}\")\n",
    "    print(f\"Manual Salinity coef vs sklearn Salinity coef difference: {abs(theta[1] - lin_reg.coef_[0]):.10f}\")\n",
    "    print(f\"Manual Density coef vs sklearn Density coef difference: {abs(theta[2] - lin_reg.coef_[1]):.10f}\")\n",
    "except NameError:\n",
    "    print(\"\\nNote: Cannot compare with manual calculations from Part C as they are not in memory.\")\n",
    "    print(\"Run Part C and Part D in the same session to see comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:41.297040Z",
     "start_time": "2025-04-03T18:36:40.273587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Residuals Plot\n",
    "residuals = y_test - y_pred_sklearn\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_sklearn, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs Predicted Values (sklearn)', fontsize=15)\n",
    "plt.xlabel('Predicted Temperature (°C)', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('residuals_plot_sklearn.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T18:36:41.478897Z",
     "start_time": "2025-04-03T18:36:41.314647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "features = X_train.columns\n",
    "importance = np.abs(lin_reg.coef_)\n",
    "importance_normalized = importance / np.sum(importance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(features, importance_normalized)\n",
    "plt.title('Feature Importance in Temperature Prediction', fontsize=15)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Normalized Importance', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E: Conceptual Questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Why is it important to have a test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set allows to evaluate the model's performance on data it hasn't seen during training, providing an unbiased assessment of how well it will generalize to new, unseen data. The performance metrics of R2 = 0.9874 and MSE = 0.2230 from the test data give you the confidence about how the model would perform in real-world applications. The test set provides a consistent benchmark for comparing different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. If the normal equation always provides a solution, when would we not want to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equation is not recomended when:\n",
    "\n",
    "- You have large datasets, for the oceanographic data with 812,168 rows, the Normal Equation requires computing (X^T X)^(-1), which has computational complexity and becomes expensive with very large datasets.\n",
    "\n",
    "- The normal equation requires holding the entire dataset in memory, which may not be feasible for extremely large datasets.\n",
    "\n",
    "- When features are highly correlated like with salinity and water density, the matrix X^T X can be close to singular, making the inverse numerically unstable.\n",
    "\n",
    "- The normal equation isn't suitable for scenarios where data arrives sequentially and the model needs continous updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How might we improve the fit of our models from Part C & D?\n",
    "\n",
    "Note: There are lots of possible answers to this section - just describe one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the residual plots, there's a clear pattern showing non-linear relationships between the predicted values and the residuals. To improve model fit of the curved pattern in the residuals, I could add polynomial features like squared or cubed terms of Salinity and STheta to capture the non-linear relationships. This would allow your linear model to fit non-linear relationships, potentially reducing both the MSE and the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. As we move further into Machine Learning, we will need to continually consider the bias-variance tradeoff. Explain what bias is and what variance is in regards to the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias is the error introduced by approximating a real-world problem with a simplified model. High bias means the model makes strong assumptions about the data and can't capture the true relationship, leading to underfitting.\n",
    "\n",
    "Variance refers to the model's sensitivity to small fluctuations in the training data. High variance means the model is too complex and captures noise in the training data rather than the underlying pattern, leading to overfitting.\n",
    "\n",
    "The bias/variance tradeoff acknowledges that as you decrease bias by using more complex models, you typically increase variance. The goal is to find the sweet spot where the total error is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. In a linear regression model, how might we reduce bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce bias in linear regression I can:\n",
    "\n",
    "- Include additional relevant features beyond Salinity and STheta.\n",
    "- Include squared or interaction terms.\n",
    "- Apply non-linear transformations to features like log and square root.\n",
    "- Move to more flexible models like decision trees or neural networks if the linear regression is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. In a linear regression model, how might we reduce variance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce variance in linear regression I can:\n",
    "\n",
    "- Apply techniques like Ridge or Lasso to constrain coefficient values.\n",
    "- Remove irrelevant or redundant features to simplify the model.\n",
    "- Increase the training data, more training examples help the model generalize better.\n",
    "- Use techniques like k-fold cross-validation to ensure model stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
