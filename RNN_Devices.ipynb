{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Recurrent Neural Network & Classification: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Processing: This data set is a bit messy, so the preprocessing portion is largely a tutorial to make sure students have data ready for keras. \n",
    "\n",
    "a) Import the following libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.layers.embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, load_model\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sequence\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.embeddings'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We will read the code in slightly differently than before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We then need to convert to a numpy.ndarray type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Check the shape of the data set - it should be (26773, 2). Spend some time looking at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Store all rows and the 0th index as the feature data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Store all rows and index 1 as the target variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) In the next step, we will clean up the predictors. This includes removing features that are not valuable, such as timestamp and source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) We next will tokenize our data, which just means vectorizing our text. Given the data we will tokenize every character (thus char_level = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Need to pad our data as each observation has a different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Create your train set to be 75% of the data and your test set to be 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model 1 - RNN: The first model will be a pretty minimal RNN with only an embedding layer, simple RNN and Dense layer. The next model we will add a few more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Start by creating an instance of a Sequential model: https://keras.io/getting-started/sequential-model-guide/Links to an external site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) From there, add an Embedding layer: https://keras.io/layers/embeddings/Links to an external site.\n",
    "\n",
    "Params:\n",
    "- input_dim = num_words (the variable we created above)\n",
    "- output_dim = 32\n",
    "- input_length = max_log_length (we also created this above)\n",
    "- Keep all other variables as the defaults (shown below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add a SimpleRNN layer: https://keras.io/layers/recurrent/Links to an external site.\n",
    "\n",
    "Params:\n",
    "- units = 32\n",
    "- activation = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Finally, we will add a Dense layer: https://keras.io/layers/core/#denseLinks to an external site.\n",
    "\n",
    "Params:\n",
    "- units = 1 (this will be our output)\n",
    "- activation --> you can choose to use either relu or sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Compile model using the .compile() method: https://keras.io/models/model/Links to an external site.\n",
    "\n",
    "Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Print the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model 2 - LSTM + Dropout Layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add a few new layers to our RNN and incorporate the more powerful LSTM. You will be creating a new model here, so make sure to call it something different than the model from Part 2.\n",
    "\n",
    "a) This RNN needs to have the following layers (add in this order):\n",
    "\n",
    "- Embedding Layer (use same params as before)\n",
    "- LSTM Layer (units = 64, recurrent_dropout = 0.5)\n",
    "- Dropout Layer - use a value of 0.5\n",
    "- Dense Layer - (use same params as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compile model using the .compile() method:\n",
    "\n",
    "Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Recurrent Neural Net Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create your RNN based on what you have learned from Model 1 & Model 2:\n",
    "\n",
    "a) RNN Requirements:\n",
    "- Use 5 or more layers\n",
    "- Add a layer that was not utilized in Model 1 or Model 2 (Note: This could be a new Dense layer or an additional LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compiler Requirements:\n",
    "- Try a new optimizer for the compile step\n",
    "- Keep accuracy as a metric (feel free to add more metrics if desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Explain the difference between the relu activation function and the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Describe what one epoch actually is (epoch was a parameter used in the .fit() method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Explain how dropout works (you can look at the keras code and/or documentation) for (a) training, and (b) test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Explain why problems such as this homework assignment are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Explain what RNN problem is solved using LSTM and briefly describe how."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
