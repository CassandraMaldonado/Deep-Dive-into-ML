{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Recurrent Neural Network & Classification: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Processing: This data set is a bit messy, so the preprocessing portion is largely a tutorial to make sure students have data ready for keras. \n",
    "\n",
    "a) Import the following libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We will read the code in slightly differently than before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We then need to convert to a numpy.ndarray type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Check the shape of the data set - it should be (26773, 2). Spend some time looking at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (26773, 2)\n",
      "Number of rows:  26773\n",
      "Number of columns:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataset: \", dataset.shape)\n",
    "print(\"Number of rows: \", dataset.shape[0])\n",
    "print(\"Number of columns: \", dataset.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Store all rows and the 0th index as the feature data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Store all rows and index 1 as the target variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) In the next step, we will clean up the predictors. This includes removing features that are not valuable, such as timestamp and source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) We next will tokenize our data, which just means vectorizing our text. Given the data we will tokenize every character (thus char_level = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Need to pad our data as each observation has a different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Create your train set to be 75% of the data and your test set to be 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set:  (20079, 1024)\n",
      "Shape of the test set:  (6694, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X_processed) * 0.75)\n",
    "X_train = X_processed[:train_size]\n",
    "X_test = X_processed[train_size:]\n",
    "Y_train = Y[:train_size]\n",
    "Y_test = Y[train_size:]\n",
    "\n",
    "print(\"Shape of the training set: \", X_train.shape)\n",
    "print(\"Shape of the test set: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model 1 - RNN: The first model will be a pretty minimal RNN with only an embedding layer, simple RNN and Dense layer. The next model we will add a few more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Start by creating an instance of a Sequential model: https://keras.io/getting-started/sequential-model-guide/Links to an external site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) From there, add an Embedding layer: https://keras.io/layers/embeddings/Links to an external site.\n",
    "\n",
    "Params:\n",
    "- input_dim = num_words (the variable we created above)\n",
    "- output_dim = 32\n",
    "- input_length = max_log_length (we also created this above)\n",
    "- Keep all other variables as the defaults (shown below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add a SimpleRNN layer: https://keras.io/layers/recurrent/Links to an external site.\n",
    "\n",
    "Params:\n",
    "- units = 32\n",
    "- activation = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Finally, we will add a Dense layer: https://keras.io/layers/core/#denseLinks to an external site.\n",
    "\n",
    "Params:\n",
    "- units = 1 (this will be our output)\n",
    "- activation --> you can choose to use either relu or sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Compile model using the .compile() method: https://keras.io/models/model/Links to an external site.\n",
    "\n",
    "Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Print the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model 2 - LSTM + Dropout Layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add a few new layers to our RNN and incorporate the more powerful LSTM. You will be creating a new model here, so make sure to call it something different than the model from Part 2.\n",
    "\n",
    "a) This RNN needs to have the following layers (add in this order):\n",
    "\n",
    "- Embedding Layer (use same params as before)\n",
    "- LSTM Layer (units = 64, recurrent_dropout = 0.5)\n",
    "- Dropout Layer - use a value of 0.5\n",
    "- Dense Layer - (use same params as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compile model using the .compile() method:\n",
    "\n",
    "Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Recurrent Neural Net Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create your RNN based on what you have learned from Model 1 & Model 2:\n",
    "\n",
    "a) RNN Requirements:\n",
    "- Use 5 or more layers\n",
    "- Add a layer that was not utilized in Model 1 or Model 2 (Note: This could be a new Dense layer or an additional LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compiler Requirements:\n",
    "- Try a new optimizer for the compile step\n",
    "- Keep accuracy as a metric (feel free to add more metrics if desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Print the model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Explain the difference between the relu activation function and the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The ReLU activation function just outputs the input if it’s positive, and 0 if it’s negative. So it’s like:_\n",
    "_ReLU(x) = max(0, x)_\n",
    "\n",
    "_The sigmoid function squishes the input into a range between 0 and 1 using a curve that looks like an “S”:_\n",
    "_Sigmoid(x) = 1 / (1 + e^(-x))_\n",
    "\n",
    "_ The main differences are:_\n",
    "_1. ReLU is super simple and fast, and it doesn’t squish big values down._\n",
    "_2. Sigmoid is good when we need outputs between 0 and 1 like probabilities, but it can cause the vanishing gradient problem in deep networks, making learning slower._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Describe what one epoch actually is (epoch was a parameter used in the .fit() method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_An epoch is one complete pass through the entire training dataset._\n",
    "\n",
    "_For example, if I have 1,000 examples, and I run fit with 5 epochs, it means the model will see all 1.000 examples 5 times during training just in different shuffled orders each time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Explain how dropout works (you can look at the keras code and/or documentation) for (a) training, and (b) test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dropout is a way to prevent overfitting by randomly turning off some neurons during training._\n",
    "\n",
    "_During training dropout randomly sets some neurons outputs to 0. This forces the model to not depend too much on any one neuron and helps it generalize better._\n",
    "\n",
    "_During testing the dropout is turned off and all neurons are used. But the output is scaled to make up for the fact that no neurons are dropped._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Explain why problems such as this homework assignment are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###......\n",
    "\n",
    "_CNNs are better for images or spatial data, where local patterns like edges or shapes are more important than the order of the input. So CNNs will usually outperform RNNs on problems like image classification or object detection._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Explain what RNN problem is solved using LSTM and briefly describe how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_RNNs can have trouble remembering things from far back in the sequence. This is called the vanishing gradient problem, it means the memory kind of fades as we go deeper into the sequence like in very large sentences._\n",
    "\n",
    "_LSTMs fix this by adding a memory cell that can store information for a long time. It uses gates to decide:_\n",
    "\n",
    "_- What to keep_\n",
    "\n",
    "_- What to throw away_\n",
    "\n",
    "_- And what new stuff to add_\n",
    "\n",
    "_This helps the model remember important info from earlier in the sequence, which regular RNNs often forgets._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
